{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "from utils import AssessData\n",
        "from datasets import load_dataset, load_metric\n",
        "#from sklearn.datasets import fetch_20newsgroups #encountered issues SSLCertificationError with this \n",
        "from datasets import load_dataset\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding\n",
        "\n",
        "from spacy import tokenizer\n",
        "from spacy.lang.en import English\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoTokenizer,AdamW,get_scheduler\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7TYEV2d-VSFI"
      },
      "outputs": [],
      "source": [
        "f1_score = load_metric(\"f1\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, label = eval_pred\n",
        "    predictions = np.argmax(predictions, axis = 1)\n",
        "    return f1_score.compute(predictions = predictions, references = label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "gxzPkdQ1Vsjp"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using custom data configuration bypublisher-data_dir=%2Fdata\n",
            "Reusing dataset hyperpartisan_news_detection (/Users/max/.cache/huggingface/datasets/hyperpartisan_news_detection/bypublisher-data_dir=%2Fdata/1.0.0/7f4215b0474950ddf516e806400ab81d098b3da3b3a919a13cd1a4cf2c677012)\n",
            "100%|██████████| 2/2 [00:00<00:00, 25.08it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "dict_keys(['train', 'validation'])"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "hyperpartisan_dataset = load_dataset(\"hyperpartisan_news_detection\", \"bypublisher\", \"/data\") #Not stripped of headers and footers\n",
        "hyperpartisan_dataset.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "distribution_name = \"hyperpartisan_distribution\"\n",
        "\n",
        "with open(distribution_name, 'r') as ins:\n",
        "    distribution= json.load(ins)\n",
        "\n",
        "seed = distribution['seed']\n",
        "\n",
        "train_index = distribution['train']\n",
        "eval_index = distribution['eval']\n",
        "test_index = distribution['test']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_index_for_longformers = train_index['Short'] + train_index[\"BERT\"] + train_index['Long']\n",
        "eval_index_for_longformers = eval_index['Short'] + eval_index[\"BERT\"] + eval_index['Long']\n",
        "test_index_for_longformers = test_index['Short'] + test_index[\"BERT\"] + test_index['Long']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCWn_3zaWr-Z",
        "outputId": "4abb2b2a-88d2-4cb5-8b72-a62fb1ace35a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading cached shuffled indices for dataset at /Users/max/.cache/huggingface/datasets/hyperpartisan_news_detection/bypublisher-data_dir=%2Fdata/1.0.0/7f4215b0474950ddf516e806400ab81d098b3da3b3a919a13cd1a4cf2c677012/cache-fd41281fa7a8783e.arrow\n",
            "Loading cached shuffled indices for dataset at /Users/max/.cache/huggingface/datasets/hyperpartisan_news_detection/bypublisher-data_dir=%2Fdata/1.0.0/7f4215b0474950ddf516e806400ab81d098b3da3b3a919a13cd1a4cf2c677012/cache-ef73cd395e00d405.arrow\n"
          ]
        }
      ],
      "source": [
        "#change to bypublisher because byarticle has no test set.\n",
        "hyperpartisan_train = hyperpartisan_dataset['train']\n",
        "hyperpartisan_valid = hyperpartisan_dataset['validation']\n",
        "\n",
        "hyperpartisan_train = hyperpartisan_train.shuffle(seed = seed)\n",
        "hyperpartisan_valid = hyperpartisan_valid.shuffle(seed = seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "S4ZiGtjGP3sn"
      },
      "outputs": [],
      "source": [
        "#Split validation data into test and train_set\n",
        "eval_test_split = hyperpartisan_valid.train_test_split(test_size = 0.5)\n",
        "\n",
        "hyperpartisan_eval = eval_test_split['train']\n",
        "hyperpartisan_test = eval_test_split['test']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "U98sIHbWflsW"
      },
      "outputs": [],
      "source": [
        "int_labels_train = [1 if x == True else 0 for x in hyperpartisan_train['hyperpartisan']]\n",
        "int_labels_eval = [1 if x == True else 0 for x in hyperpartisan_eval['hyperpartisan']]\n",
        "int_labels_test = [1 if x == True else 0 for x in hyperpartisan_test['hyperpartisan']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading cached processed dataset at /Users/max/.cache/huggingface/datasets/hyperpartisan_news_detection/bypublisher-data_dir=%2Fdata/1.0.0/7f4215b0474950ddf516e806400ab81d098b3da3b3a919a13cd1a4cf2c677012/cache-6e59f494bef5bf5e.arrow\n",
            "Flattening the indices: 100%|██████████| 300/300 [00:36<00:00,  8.14ba/s]\n",
            "Flattening the indices: 100%|██████████| 300/300 [00:34<00:00,  8.73ba/s]\n"
          ]
        }
      ],
      "source": [
        "hyperpartisan_train = hyperpartisan_train.add_column(\"labels\", int_labels_train)\n",
        "hyperpartisan_eval= hyperpartisan_eval.add_column(\"labels\", int_labels_eval)\n",
        "hyperpartisan_test = hyperpartisan_test.add_column(\"labels\", int_labels_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "hyperpartisan_train = hyperpartisan_train.remove_columns([\"title\",\"hyperpartisan\",\"url\",\"published_at\",\"bias\"])\n",
        "hyperpartisan_eval = hyperpartisan_eval.remove_columns([\"title\",\"hyperpartisan\",\"url\",\"published_at\",\"bias\"])\n",
        "hyperpartisan_test = hyperpartisan_test.remove_columns([\"title\",\"hyperpartisan\",\"url\",\"published_at\",\"bias\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "hyperpartisan_train = hyperpartisan_train.select(train_index_for_longformers)\n",
        "hyperpartisan_eval = hyperpartisan_eval.select(eval_index_for_longformers)\n",
        "hyperpartisan_test = hyperpartisan_test.select(test_index_for_longformers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#extract long documents from test and eval\n",
        "#evaluate on test and eval\n",
        "#Use test_json to have index of long_files and short files and normal files as well as labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "SFJuKj0YhpH7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: 100%|██████████| 570M/570M [00:38<00:00, 15.5MB/s] \n",
            "Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias']\n",
            "- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "#FINETUNING TO A NUMBER OF Labels\n",
        "from transformers import LongformerTokenizer, LongformerForSequenceClassification,LongformerConfig\n",
        "\n",
        "longformer_tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')\n",
        "\n",
        "longformer_model = LongformerForSequenceClassification.from_pretrained('allenai/longformer-base-4096',\n",
        "                                                  gradient_checkpointing=False,\n",
        "                                                  attention_window = 512,\n",
        "                                                  num_labels = 2,\n",
        "                                                  return_dict=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oy_kDz6UMbp6"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "  input = longformer_tokenizer(hyperpartisan_train['text'], return_tensors= \"pt\")\n",
        "  out = longformer_model(**input).logits\n",
        "\n",
        "  labels = out.argmax().item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iYA16tNZO3eL"
      },
      "outputs": [],
      "source": [
        "labels, hyperpartisan_train['hyperpartisan']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4noTHvISMul"
      },
      "source": [
        "FINETUNING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "E6S1CtEkTBsK"
      },
      "outputs": [],
      "source": [
        "import optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "YtOREa72S_Mi"
      },
      "outputs": [],
      "source": [
        "def hp_space(trial):\n",
        "  return {\"per_device_train_batch_size\":trial.suggest_int(\"per_device_train_batch_size\", 8,16,8),\n",
        "          \"num_train_epochs\":trial.suggest_int(\"num_training_epochs\",1,5),\n",
        "          \"learning_rate\": trial.suggest_float(\"learning_rate\",1e-5,2e-5),\n",
        "  }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "R6MVxq7ZvCGB",
        "outputId": "a1794c61-d138-40d1-e29d-b8f595c0200b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "/Users/max/Desktop/Dissertation/implementations/Disso-COLD/proj_env/lib/python3.8/site-packages/transformers/trainer.py:330: FutureWarning: `Trainer` requires either a `model` or `model_init` argument, but not both. `model_init` will overwrite your model when calling the `train` method. This will become a fatal error in the next release.\n",
            "  warnings.warn(\n",
            "  0%|          | 0/73606 [18:45<?, ?it/s]\n",
            "\u001b[32m[I 2022-07-28 13:26:26,879]\u001b[0m A new study created in memory with name: no-name-e40b98a4-3e92-4db5-9728-e3a5a60683b5\u001b[0m\n",
            "Trial:\n",
            "loading configuration file https://huggingface.co/allenai/longformer-base-4096/resolve/main/config.json from cache at /Users/max/.cache/huggingface/transformers/0690955d8f70934f95adf0fb108d5f7322d02f8d7dd938b7b133cb7421e120e6.b25f41ff6acdcb7ab47c505c70e351b3fc01957b3798197e5ac6e8efc547ac99\n",
            "Model config LongformerConfig {\n",
            "  \"attention_mode\": \"longformer\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"attention_window\": 512,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"ignore_attention_mask\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 4098,\n",
            "  \"model_type\": \"longformer\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"sep_token_id\": 2,\n",
            "  \"transformers_version\": \"4.19.4\",\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/allenai/longformer-base-4096/resolve/main/pytorch_model.bin from cache at /Users/max/.cache/huggingface/transformers/a7a586602e625bd012d75abdfcc615f5bb1fe133273845f7381332c634273bd9.dc3a4f03d4ab11f972b126d0e6b67f43e5d9003b3aec54f8e549cc7e2d42398d\n",
            "Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias']\n",
            "- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "The following columns in the training set don't have a corresponding argument in `LongformerForSequenceClassification.forward` and have been ignored: text. If text are not expected by `LongformerForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "/Users/max/Desktop/Dissertation/implementations/Disso-COLD/proj_env/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "***** Running training *****\n",
            "  Num examples = 588846\n",
            "  Num Epochs = 3\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 110409\n",
            "  0%|          | 0/110409 [00:00<?, ?it/s]\u001b[33m[W 2022-07-28 13:26:29,464]\u001b[0m Trial 0 failed because of the following error: ValueError(\"You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided ['labels']\")\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/max/Desktop/Dissertation/implementations/Disso-COLD/proj_env/lib/python3.8/site-packages/optuna/study/_optimize.py\", line 213, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "  File \"/Users/max/Desktop/Dissertation/implementations/Disso-COLD/proj_env/lib/python3.8/site-packages/transformers/integrations.py\", line 156, in _objective\n",
            "    trainer.train(resume_from_checkpoint=checkpoint, trial=trial)\n",
            "  File \"/Users/max/Desktop/Dissertation/implementations/Disso-COLD/proj_env/lib/python3.8/site-packages/transformers/trainer.py\", line 1317, in train\n",
            "    return inner_training_loop(\n",
            "  File \"/Users/max/Desktop/Dissertation/implementations/Disso-COLD/proj_env/lib/python3.8/site-packages/transformers/trainer.py\", line 1528, in _inner_training_loop\n",
            "    for step, inputs in enumerate(epoch_iterator):\n",
            "  File \"/Users/max/Desktop/Dissertation/implementations/Disso-COLD/proj_env/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 530, in __next__\n",
            "    data = self._next_data()\n",
            "  File \"/Users/max/Desktop/Dissertation/implementations/Disso-COLD/proj_env/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 570, in _next_data\n",
            "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
            "  File \"/Users/max/Desktop/Dissertation/implementations/Disso-COLD/proj_env/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n",
            "    return self.collate_fn(data)\n",
            "  File \"/Users/max/Desktop/Dissertation/implementations/Disso-COLD/proj_env/lib/python3.8/site-packages/transformers/data/data_collator.py\", line 247, in __call__\n",
            "    batch = self.tokenizer.pad(\n",
            "  File \"/Users/max/Desktop/Dissertation/implementations/Disso-COLD/proj_env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py\", line 2795, in pad\n",
            "    raise ValueError(\n",
            "ValueError: You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided ['labels']\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided ['labels']",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32m/Users/max/Desktop/Dissertation/implementations/Disso-COLD/longformer_v2.ipynb Cell 20\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/max/Desktop/Dissertation/implementations/Disso-COLD/longformer_v2.ipynb#ch0000021?line=14'>15</a>\u001b[0m longformer_finetuned \u001b[39m=\u001b[39m Trainer(model_init \u001b[39m=\u001b[39m longformer_init, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/max/Desktop/Dissertation/implementations/Disso-COLD/longformer_v2.ipynb#ch0000021?line=15'>16</a>\u001b[0m               model \u001b[39m=\u001b[39m longformer_model,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/max/Desktop/Dissertation/implementations/Disso-COLD/longformer_v2.ipynb#ch0000021?line=16'>17</a>\u001b[0m               args \u001b[39m=\u001b[39m training_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/max/Desktop/Dissertation/implementations/Disso-COLD/longformer_v2.ipynb#ch0000021?line=20'>21</a>\u001b[0m               data_collator\u001b[39m=\u001b[39mdata_collator,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/max/Desktop/Dissertation/implementations/Disso-COLD/longformer_v2.ipynb#ch0000021?line=21'>22</a>\u001b[0m               compute_metrics\u001b[39m=\u001b[39m compute_metrics)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/max/Desktop/Dissertation/implementations/Disso-COLD/longformer_v2.ipynb#ch0000021?line=23'>24</a>\u001b[0m \u001b[39m#searching for best hyperparameters\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/max/Desktop/Dissertation/implementations/Disso-COLD/longformer_v2.ipynb#ch0000021?line=24'>25</a>\u001b[0m best_run \u001b[39m=\u001b[39m longformer_finetuned\u001b[39m.\u001b[39;49mhyperparameter_search(direction \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m\"\u001b[39;49m, hp_space \u001b[39m=\u001b[39;49m hp_space)\n",
            "File \u001b[0;32m~/Desktop/Dissertation/implementations/Disso-COLD/proj_env/lib/python3.8/site-packages/transformers/trainer.py:2085\u001b[0m, in \u001b[0;36mTrainer.hyperparameter_search\u001b[0;34m(self, hp_space, compute_objective, n_trials, direction, backend, hp_name, **kwargs)\u001b[0m\n\u001b[1;32m   2077\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_objective \u001b[39m=\u001b[39m default_compute_objective \u001b[39mif\u001b[39;00m compute_objective \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m compute_objective\n\u001b[1;32m   2079\u001b[0m backend_dict \u001b[39m=\u001b[39m {\n\u001b[1;32m   2080\u001b[0m     HPSearchBackend\u001b[39m.\u001b[39mOPTUNA: run_hp_search_optuna,\n\u001b[1;32m   2081\u001b[0m     HPSearchBackend\u001b[39m.\u001b[39mRAY: run_hp_search_ray,\n\u001b[1;32m   2082\u001b[0m     HPSearchBackend\u001b[39m.\u001b[39mSIGOPT: run_hp_search_sigopt,\n\u001b[1;32m   2083\u001b[0m     HPSearchBackend\u001b[39m.\u001b[39mWANDB: run_hp_search_wandb,\n\u001b[1;32m   2084\u001b[0m }\n\u001b[0;32m-> 2085\u001b[0m best_run \u001b[39m=\u001b[39m backend_dict[backend](\u001b[39mself\u001b[39;49m, n_trials, direction, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   2087\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhp_search_backend \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   2088\u001b[0m \u001b[39mreturn\u001b[39;00m best_run\n",
            "File \u001b[0;32m~/Desktop/Dissertation/implementations/Disso-COLD/proj_env/lib/python3.8/site-packages/transformers/integrations.py:166\u001b[0m, in \u001b[0;36mrun_hp_search_optuna\u001b[0;34m(trainer, n_trials, direction, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m n_jobs \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mn_jobs\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m    165\u001b[0m study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(direction\u001b[39m=\u001b[39mdirection, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 166\u001b[0m study\u001b[39m.\u001b[39;49moptimize(_objective, n_trials\u001b[39m=\u001b[39;49mn_trials, timeout\u001b[39m=\u001b[39;49mtimeout, n_jobs\u001b[39m=\u001b[39;49mn_jobs)\n\u001b[1;32m    167\u001b[0m best_trial \u001b[39m=\u001b[39m study\u001b[39m.\u001b[39mbest_trial\n\u001b[1;32m    168\u001b[0m \u001b[39mreturn\u001b[39;00m BestRun(\u001b[39mstr\u001b[39m(best_trial\u001b[39m.\u001b[39mnumber), best_trial\u001b[39m.\u001b[39mvalue, best_trial\u001b[39m.\u001b[39mparams)\n",
            "File \u001b[0;32m~/Desktop/Dissertation/implementations/Disso-COLD/proj_env/lib/python3.8/site-packages/optuna/study/study.py:400\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[39mif\u001b[39;00m n_jobs \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    393\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    394\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`n_jobs` argument has been deprecated in v2.7.0. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    395\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThis feature will be removed in v4.0.0. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    396\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mSee https://github.com/optuna/optuna/releases/tag/v2.7.0.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    397\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    398\u001b[0m     )\n\u001b[0;32m--> 400\u001b[0m _optimize(\n\u001b[1;32m    401\u001b[0m     study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[1;32m    402\u001b[0m     func\u001b[39m=\u001b[39;49mfunc,\n\u001b[1;32m    403\u001b[0m     n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[1;32m    404\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    405\u001b[0m     n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[1;32m    406\u001b[0m     catch\u001b[39m=\u001b[39;49mcatch,\n\u001b[1;32m    407\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    408\u001b[0m     gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[1;32m    409\u001b[0m     show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[1;32m    410\u001b[0m )\n",
            "File \u001b[0;32m~/Desktop/Dissertation/implementations/Disso-COLD/proj_env/lib/python3.8/site-packages/optuna/study/_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[1;32m     67\u001b[0m             study,\n\u001b[1;32m     68\u001b[0m             func,\n\u001b[1;32m     69\u001b[0m             n_trials,\n\u001b[1;32m     70\u001b[0m             timeout,\n\u001b[1;32m     71\u001b[0m             catch,\n\u001b[1;32m     72\u001b[0m             callbacks,\n\u001b[1;32m     73\u001b[0m             gc_after_trial,\n\u001b[1;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m     77\u001b[0m         )\n\u001b[1;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m show_progress_bar:\n",
            "File \u001b[0;32m~/Desktop/Dissertation/implementations/Disso-COLD/proj_env/lib/python3.8/site-packages/optuna/study/_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[1;32m    164\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
            "File \u001b[0;32m~/Desktop/Dissertation/implementations/Disso-COLD/proj_env/lib/python3.8/site-packages/optuna/study/_optimize.py:264\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    263\u001b[0m \u001b[39mif\u001b[39;00m state \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch):\n\u001b[0;32m--> 264\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[1;32m    265\u001b[0m \u001b[39mreturn\u001b[39;00m trial\n",
            "File \u001b[0;32m~/Desktop/Dissertation/implementations/Disso-COLD/proj_env/lib/python3.8/site-packages/optuna/study/_optimize.py:213\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    210\u001b[0m     thread\u001b[39m.\u001b[39mstart()\n\u001b[1;32m    212\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 213\u001b[0m     value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[1;32m    214\u001b[0m \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
            "File \u001b[0;32m~/Desktop/Dissertation/implementations/Disso-COLD/proj_env/lib/python3.8/site-packages/transformers/integrations.py:156\u001b[0m, in \u001b[0;36mrun_hp_search_optuna.<locals>._objective\u001b[0;34m(trial, checkpoint_dir)\u001b[0m\n\u001b[1;32m    154\u001b[0m             checkpoint \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(checkpoint_dir, subdir)\n\u001b[1;32m    155\u001b[0m trainer\u001b[39m.\u001b[39mobjective \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 156\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain(resume_from_checkpoint\u001b[39m=\u001b[39;49mcheckpoint, trial\u001b[39m=\u001b[39;49mtrial)\n\u001b[1;32m    157\u001b[0m \u001b[39m# If there hasn't been any evaluation during the training loop.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(trainer, \u001b[39m\"\u001b[39m\u001b[39mobjective\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "File \u001b[0;32m~/Desktop/Dissertation/implementations/Disso-COLD/proj_env/lib/python3.8/site-packages/transformers/trainer.py:1317\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1312\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m   1314\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1315\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1316\u001b[0m )\n\u001b[0;32m-> 1317\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1318\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1319\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1320\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1321\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1322\u001b[0m )\n",
            "File \u001b[0;32m~/Desktop/Dissertation/implementations/Disso-COLD/proj_env/lib/python3.8/site-packages/transformers/trainer.py:1528\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1525\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_load_rng_state(resume_from_checkpoint)\n\u001b[1;32m   1527\u001b[0m step \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[0;32m-> 1528\u001b[0m \u001b[39mfor\u001b[39;00m step, inputs \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(epoch_iterator):\n\u001b[1;32m   1529\u001b[0m \n\u001b[1;32m   1530\u001b[0m     \u001b[39m# Skip past any already trained steps if resuming training\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m     \u001b[39mif\u001b[39;00m steps_trained_in_current_epoch \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1532\u001b[0m         steps_trained_in_current_epoch \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
            "File \u001b[0;32m~/Desktop/Dissertation/implementations/Disso-COLD/proj_env/lib/python3.8/site-packages/torch/utils/data/dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    529\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[0;32m--> 530\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    531\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    532\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    533\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    534\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
            "File \u001b[0;32m~/Desktop/Dissertation/implementations/Disso-COLD/proj_env/lib/python3.8/site-packages/torch/utils/data/dataloader.py:570\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    569\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 570\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    571\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    572\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data)\n",
            "File \u001b[0;32m~/Desktop/Dissertation/implementations/Disso-COLD/proj_env/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 52\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
            "File \u001b[0;32m~/Desktop/Dissertation/implementations/Disso-COLD/proj_env/lib/python3.8/site-packages/transformers/data/data_collator.py:247\u001b[0m, in \u001b[0;36mDataCollatorWithPadding.__call__\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, features: List[Dict[\u001b[39mstr\u001b[39m, Any]]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, Any]:\n\u001b[0;32m--> 247\u001b[0m     batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49mpad(\n\u001b[1;32m    248\u001b[0m         features,\n\u001b[1;32m    249\u001b[0m         padding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding,\n\u001b[1;32m    250\u001b[0m         max_length\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_length,\n\u001b[1;32m    251\u001b[0m         pad_to_multiple_of\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m    252\u001b[0m         return_tensors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreturn_tensors,\n\u001b[1;32m    253\u001b[0m     )\n\u001b[1;32m    254\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m batch:\n\u001b[1;32m    255\u001b[0m         batch[\u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m batch[\u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m]\n",
            "File \u001b[0;32m~/Desktop/Dissertation/implementations/Disso-COLD/proj_env/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2795\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.pad\u001b[0;34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, return_attention_mask, return_tensors, verbose)\u001b[0m\n\u001b[1;32m   2793\u001b[0m \u001b[39m# The model's main input name, usually `input_ids`, has be passed for padding\u001b[39;00m\n\u001b[1;32m   2794\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_input_names[\u001b[39m0\u001b[39m] \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m encoded_inputs:\n\u001b[0;32m-> 2795\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2796\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou should supply an encoding or a list of encodings to this method \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2797\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mthat includes \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_input_names[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m, but you provided \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlist\u001b[39m(encoded_inputs\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2798\u001b[0m     )\n\u001b[1;32m   2800\u001b[0m required_input \u001b[39m=\u001b[39m encoded_inputs[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_input_names[\u001b[39m0\u001b[39m]]\n\u001b[1;32m   2802\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m required_input:\n",
            "\u001b[0;31mValueError\u001b[0m: You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided ['labels']"
          ]
        }
      ],
      "source": [
        "\n",
        "def longformer_init():\n",
        "  return (LongformerForSequenceClassification.from_pretrained('allenai/longformer-base-4096',\n",
        "                                                  gradient_checkpointing=False,\n",
        "                                                  attention_window = 512,\n",
        "                                                  num_labels = 2,\n",
        "                                                  return_dict=True))\n",
        "                                                                     \n",
        "data_collator = DataCollatorWithPadding(tokenizer=longformer_tokenizer) #to pad sequences\n",
        "\n",
        "#default training arguments \n",
        "training_args = TrainingArguments(output_dir=\"./results\", per_device_train_batch_size=16, \n",
        "                                  evaluation_strategy= \"epoch\", per_device_eval_batch_size=16, num_train_epochs=5)\n",
        "\n",
        "#fine-tuning model\n",
        "longformer_finetuned = Trainer(model_init = longformer_init, \n",
        "              model = longformer_model,\n",
        "              args = training_args,\n",
        "              train_dataset= hyperpartisan_train,\n",
        "              eval_dataset= hyperpartisan_eval,\n",
        "              tokenizer= longformer_tokenizer,\n",
        "              data_collator=data_collator,\n",
        "              compute_metrics= compute_metrics)\n",
        "\n",
        "#searching for best hyperparameters\n",
        "best_run = longformer_finetuned.hyperparameter_search(direction = \"maximize\", hp_space = hp_space)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uqIoWYEpYjje"
      },
      "outputs": [],
      "source": [
        "#plot \n",
        "fig, ax = optuna.visualization.plot_slice(study, params=[\"x\", \"y\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pDLHZS-4C4hB"
      },
      "outputs": [],
      "source": [
        "#update hyperparameters \n",
        "for k,v in best_run.hyperparameters.items():\n",
        "  setattr(training_args, k, v)\n",
        "\n",
        "best_trainer = Trainer(model = longformer_model,\n",
        "                       args = training_args,\n",
        "                       train_dataset= hyperpartisan_train,\n",
        "                      eval_dataset= hyperpartisan_eval,\n",
        "                      tokenizer= longformer_tokenizer,\n",
        "                      data_collator=data_collator,\n",
        "                      compute_metrics= compute_metrics)\n",
        "\n",
        "best_trainer.train()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "longformer.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.2 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.2"
    },
    "vscode": {
      "interpreter": {
        "hash": "7e1998ff7f8aa20ada591c520b972326324e5ea05489af9e422744c7c09f6dad"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
